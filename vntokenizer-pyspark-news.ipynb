{
 "metadata": {
  "name": "",
  "signature": "sha256:e4b8d239f347c6131c07acf5c9997ab5150ea92bc7d3687afef2c925eb5e3fd2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext\n",
      "from py4j.java_gateway import java_import\n",
      "from pyspark.mllib.common import _to_java_object_rdd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import vnTokenizer from Java\n",
      "java_import(sc._gateway.jvm, \"vn.vitk.tok.Tokenizer\")\n",
      "Tokenizer = sc._jvm.vn.vitk.tok.Tokenizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load Data set\n",
      "data = sc.textFile('./data/data_test.txt')\n",
      "data_rdd_java = _to_java_object_rdd(data) # Convert RDD to JavaRDD"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tokenize\n",
      "dataFolder = '/export/dat/tok'\n",
      "token = Tokenizer(sc._jsc, dataFolder + \"/lexicon.xml\", dataFolder + \"/regexp.txt\")\n",
      "result = token.tokenize(data_rdd_java)\n",
      "result.saveAsTextFile('./output/tokenize')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}